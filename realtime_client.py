"""Realtime multimodal client that reuses the GPT realtime API workflow."""

from __future__ import annotations

import asyncio
import base64
import json
import os
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional, Sequence

import cv2
import numpy as np
import sounddevice as sd
from websockets import connect

from object_mapper import ObjectMapper, SnapshotPayload
from system_prompt import (
    MULTI_MODAL_SYSTEM_PROMPT,
    SNAPSHOT_REQUEST_PROMPT,
    HANDOFF_PROMPT,
    DELIVERY_PROMPT,
)
from config import APP_CONFIG

sd.default.device = "default"


def test_microphone(duration: int = 2, fs: int = 24000) -> bool:
    """ë§ˆì´í¬ í…ŒìŠ¤íŠ¸"""

    print("\nğŸ¤ ë§ˆì´í¬ í…ŒìŠ¤íŠ¸ ì¤‘...")
    try:
        devices = sd.query_devices()
        print("\nğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì˜¤ë””ì˜¤ ì¥ì¹˜:")
        for idx, device in enumerate(devices):
            if device.get("max_input_channels", 0) > 0:
                print(f"  [{idx}] {device['name']} (ì…ë ¥ ì±„ë„: {device['max_input_channels']})")

        default_input = sd.query_devices(kind="input")
        print(f"\nâœ… í˜„ì¬ ê¸°ë³¸ ì…ë ¥ ì¥ì¹˜: {default_input['name']}")

        print(f"ğŸ¤ {duration}ì´ˆ í…ŒìŠ¤íŠ¸ ë…¹ìŒ ì‹œì‘...")
        recording = sd.rec(int(fs * duration), samplerate=fs, channels=1, dtype="int16")
        sd.wait()

        audio_max = int(np.max(np.abs(recording)))
        audio_mean = float(np.mean(np.abs(recording)))
        print("\nğŸ“Š ë…¹ìŒ ë°ì´í„° ë¶„ì„:")
        print(f"   - ìµœëŒ€ ì§„í­: {audio_max}")
        print(f"   - í‰ê·  ì§„í­: {audio_mean:.2f}")

        if audio_max < 100:
            print("âš ï¸  ê²½ê³ : ë§ˆì´í¬ ì…ë ¥ì´ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤! ë§ˆì´í¬ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
            return False

        print("âœ… ë§ˆì´í¬ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤!\n")
        return True
    except Exception as exc:
        print(f"âŒ ë§ˆì´í¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {exc}")
        return False


@dataclass
class PickDecision:
    label: Optional[str]
    transcript: str
    raw_response: str


def _load_env_file() -> None:
    """Load environment variables from nearby .env files if available."""
    candidates = [
        Path(__file__).resolve().parent / ".env",
        Path.cwd() / ".env",
    ]
    for env_path in candidates:
        if not env_path.exists():
            continue
        for line in env_path.read_text(encoding="utf-8").splitlines():
            if not line or line.strip().startswith("#") or "=" not in line:
                continue
            key, value = line.split("=", 1)
            os.environ.setdefault(key.strip(), value.strip())


class RealtimeImageVoiceClient:
    """Handles the realtime websocket session for voice + image instructions."""

    def __init__(
        self,
        model: Optional[str] = None,
        instructions: str = MULTI_MODAL_SYSTEM_PROMPT,
        operator_audio_seconds: Optional[int] = None,
        use_microphone: bool = True,
        play_audio: Optional[bool] = None,
        debug_events: bool = False,
    ) -> None:
        cfg = APP_CONFIG.realtime
        _load_env_file()
        self.api_key = os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise RuntimeError("OPENAI_API_KEY is not set. Create a .env or export the variable.")

        model_name = model or cfg.model
        self.url = f"wss://api.openai.com/v1/realtime?model={model_name}"
        self.instructions = instructions.strip()
        self.operator_audio_seconds = operator_audio_seconds or cfg.operator_audio_seconds
        self.use_microphone = use_microphone
        self.play_audio = cfg.play_audio if play_audio is None else play_audio
        self.debug_events = debug_events
        self.modalities: List[str] = ["text"]
        if self.play_audio:
            self.modalities.append("audio")

        self._headers = [
            ("Authorization", f"Bearer {self.api_key}"),
            ("OpenAI-Beta", "realtime=v1"),
        ]

    def ask_snapshot_permission(self) -> bool:
        """ìŠ¤ëƒ…ìƒ· ì´¬ì˜ í—ˆê°€ ìš”ì²­"""
        return asyncio.run(self._ask_snapshot_permission())

    def request_pick_label(
        self,
        mapper: ObjectMapper,
        snapshot: SnapshotPayload,
        max_turns: Optional[int] = None,
    ) -> PickDecision:
        """Blocking helper that spins an asyncio loop until a label is extracted."""

        return asyncio.run(self._request_pick_label(mapper, snapshot, max_turns or APP_CONFIG.realtime.max_turns))

    def request_handoff_consent(self, frame_bgr: np.ndarray) -> bool:
        """Compliment the operator and ask for permission to hand over the object."""
        return asyncio.run(self._request_handoff_consent(frame_bgr))

    def request_delivery_destination(self, item_description: str) -> str:
        """Ask operator where to deliver the object. Returns 'person' or 'basket'."""
        return asyncio.run(self._request_delivery_destination(item_description))

    async def _ask_snapshot_permission(self) -> bool:
        """ìŠ¤ëƒ…ìƒ· ì´¬ì˜ í—ˆê°€ë¥¼ AIì—ê²Œ ë¬¼ì–´ë´„"""
        print("[Realtime] Connecting to OpenAI realtime API...")

        # ì„ì‹œë¡œ SNAPSHOT_REQUEST_PROMPTë¥¼ ì‚¬ìš©
        temp_instructions = self.instructions
        self.instructions = SNAPSHOT_REQUEST_PROMPT

        try:
            async with connect(self.url, additional_headers=self._headers) as ws:
                print("[Realtime] WebSocket connected.")
                await self._clear_conversation(ws)
                await self._send_session_update(ws)

                # AIê°€ ë¨¼ì € "ìŠ¤ëƒ…ìƒ·ì„ ì°ì„ê¹Œìš”?" ë¬¼ì–´ë´„
                await self._request_response(ws)
                print("\nğŸ¤– AI: ", end="", flush=True)
                await self._drain_response(ws, stream_print=True)

                # ì‚¬ìš©ì ì‘ë‹µ ë°›ê¸°
                if not await self._send_operator_turn(ws, 0):
                    return False

                await self._request_response(ws)
                print("\nğŸ¤– AI: ", end="", flush=True)
                transcript = await self._drain_response(ws, stream_print=True)

                # TAKE_SNAPSHOT: YES í™•ì¸
                if "TAKE_SNAPSHOT:" in transcript and "YES" in transcript:
                    return True
                return False
        finally:
            self.instructions = temp_instructions

    async def _request_pick_label(
        self,
        mapper: ObjectMapper,
        snapshot: SnapshotPayload,
        max_turns: int,
    ) -> PickDecision:
        print("[Realtime] Connecting to OpenAI realtime API...")
        async with connect(self.url, additional_headers=self._headers) as ws:
            print("[Realtime] WebSocket connected.")
            await self._clear_conversation(ws)
            await self._send_session_update(ws)

            # ì´ë¯¸ì§€ë¥¼ user ë©”ì‹œì§€ë¡œ ì „ì†¡
            await self._send_image_message(ws, snapshot)

            # AI ì‘ë‹µ ìš”ì²­
            await self._request_response(ws)
            print("\nğŸ¤– AI: ", end="", flush=True)
            initial_ack = await self._drain_response(ws, stream_print=True)

            transcript_cache = ""
            for turn in range(max_turns):
                if not await self._send_operator_turn(ws, turn):
                    continue
                await self._request_response(ws)
                print("\nğŸ¤– AI: ", end="", flush=True)
                transcript_cache = await self._drain_response(ws, stream_print=True)
                labeled_obj = mapper.resolve_label(transcript_cache, snapshot)
                if labeled_obj:
                    return PickDecision(label=labeled_obj.label, transcript=transcript_cache, raw_response=transcript_cache)

            return PickDecision(label=None, transcript=transcript_cache, raw_response=transcript_cache)

    async def _request_handoff_consent(self, frame_bgr: np.ndarray) -> bool:
        temp_instructions = self.instructions
        self.instructions = HANDOFF_PROMPT
        try:
            async with connect(self.url, additional_headers=self._headers) as ws:
                print("[Realtime] WebSocket connected for handoff consent.")
                await self._clear_conversation(ws)
                await self._send_session_update(ws)

                await self._send_person_image_message(ws, frame_bgr)

                # AI compliments + asks question
                await self._request_response(ws)
                print("\nğŸ¤– AI: ", end="", flush=True)
                await self._drain_response(ws, stream_print=True)

                # Operator response (voice/text)
                if not await self._send_operator_turn(ws, 0):
                    print("[Realtime] Operator response missing; default to NO")
                    return False

                await self._request_response(ws)
                print("\nğŸ¤– AI: ", end="", flush=True)
                transcript = await self._drain_response(ws, stream_print=True)
                return "CONSENT: YES" in transcript.upper()
        finally:
            self.instructions = temp_instructions

    async def _request_delivery_destination(self, item_description: str) -> str:
        temp_instructions = self.instructions
        self.instructions = DELIVERY_PROMPT
        try:
            async with connect(self.url, additional_headers=self._headers) as ws:
                print("[Realtime] WebSocket connected for delivery selection.")
                await self._clear_conversation(ws)
                await self._send_session_update(ws)

                await self._send_delivery_prompt(ws, item_description)

                # AI asks the question
                await self._request_response(ws)
                print("\nğŸ¤– AI: ", end="", flush=True)
                await self._drain_response(ws, stream_print=True)

                # Operator response
                if not await self._send_operator_turn(ws, 0):
                    print("[Realtime] Operator response missing; default to PERSON")
                    return "person"

                await self._request_response(ws)
                print("\nğŸ¤– AI: ", end="", flush=True)
                transcript = await self._drain_response(ws, stream_print=True)
                upper = transcript.upper()
                if "DESTINATION: BASKET" in upper:
                    return "basket"
                if "DESTINATION: PERSON" in upper:
                    return "person"
                return "person"
        finally:
            self.instructions = temp_instructions

    async def _clear_conversation(self, ws) -> None:
        await ws.send(json.dumps({"type": "conversation.clear"}))
        print("[Realtime] Conversation cleared.")

    async def _send_session_update(self, ws) -> None:
        await ws.send(
            json.dumps(
                {
                    "type": "session.update",
                    "session": {
                        "turn_detection": None,
                        "input_audio_format": "pcm16",
                        "output_audio_format": "pcm16",
                        "modalities": self.modalities,
                        "instructions": self.instructions,
                    },
                }
            )
        )

    async def _send_image_message(self, ws, snapshot: SnapshotPayload) -> None:
        """ì´ë¯¸ì§€ë¥¼ user ë©”ì‹œì§€ë¡œ ì „ì†¡"""
        data_url, mime = snapshot.to_base64(mime="image/jpeg")
        image_size_kb = len(data_url) / 1024
        print(f"[Realtime] ì´ë¯¸ì§€ ì „ì†¡: {snapshot.original_frame.shape}, {image_size_kb:.1f}KB")

        prompt_text = (
            f"ì´ë¯¸ì§€ë¥¼ ë³´ë©´ ë¬¼ì²´ ìœ„ì— ë²ˆí˜¸(1, 2, 3 ë“±)ê°€ í‘œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n"
            f"ì´ {len(snapshot.labeled_objects)}ê°œì˜ ë¬¼ì²´ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
            f"ê° ë²ˆí˜¸ì— í•´ë‹¹í•˜ëŠ” ë¬¼ì²´ê°€ ë¬´ì—‡ì¸ì§€ ê°„ë‹¨íˆ ë§í•´ì£¼ì„¸ìš”:\n"
            f"í˜•ì‹: 'OBJ-01ì€ [ë¬¼ì²´ì´ë¦„]ì…ë‹ˆë‹¤. OBJ-02ëŠ” [ë¬¼ì²´ì´ë¦„]ì…ë‹ˆë‹¤.'\n"
            f"ë§ˆì§€ë§‰ì— 'ì–´ë–¤ ë¬¼ì²´ë¥¼ ì§‘ì„ê¹Œìš”?'ë¼ê³  ë¬¼ìœ¼ì„¸ìš”."
        )

        await ws.send(
            json.dumps(
                {
                    "type": "conversation.item.create",
                    "item": {
                        "type": "message",
                        "role": "user",
                        "content": [
                            {"type": "input_text", "text": prompt_text},
                            {"type": "input_image", "image_url": data_url},
                        ],
                    },
                }
            )
        )
        print(f"[Realtime] í”„ë¡¬í”„íŠ¸: {prompt_text[:80]}...")

    async def _send_person_image_message(self, ws, frame_bgr: np.ndarray) -> None:
        data_url = _frame_to_data_url(frame_bgr)
        prompt_text = (
            "ì´ë¯¸ì§€ë¥¼ ë³´ê³  ìƒëŒ€ë°©ì˜ ì˜·ì°¨ë¦¼ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ì¹­ì°¬í•œ ë’¤, "
            "'ë¬¼ê±´ì„ ë“œë¦´ê¹Œìš”?'ë¼ê³  ê¼­ ë¬¼ì–´ë³´ì„¸ìš”."
        )
        await ws.send(
            json.dumps(
                {
                    "type": "conversation.item.create",
                    "item": {
                        "type": "message",
                        "role": "user",
                        "content": [
                            {"type": "input_text", "text": prompt_text},
                            {"type": "input_image", "image_url": data_url},
                        ],
                    },
                }
            )
        )
        print("[Realtime] ì „ì†¡ëœ ì´ë¯¸ì§€ë¡œ ì˜ìƒ ì¹­ì°¬ ë° ì „ë‹¬ ì—¬ë¶€ ì§ˆë¬¸")

    async def _send_delivery_prompt(self, ws, item_description: str) -> None:
        prompt_text = (
            f"ë°©ê¸ˆ ì„ íƒëœ ë¬¼ì²´ ì„¤ëª…: {item_description}\n"
            f"ìœ„ ë¬¼ì²´ë¥¼ ì–´ë””ë¡œ ê°€ì ¸ë‹¤ ë“œë¦´ì§€ ë¬¼ì–´ë³´ì„¸ìš”. "
            f"1ë²ˆì€ ì‚¬ëŒ(ì§ì ‘ ì „ë‹¬), 2ë²ˆì€ ë°”êµ¬ë‹ˆë¡œ ì „ë‹¬ì…ë‹ˆë‹¤."
        )
        await ws.send(
            json.dumps(
                {
                    "type": "conversation.item.create",
                    "item": {
                        "type": "message",
                        "role": "user",
                        "content": [
                            {"type": "input_text", "text": prompt_text},
                        ],
                    },
                }
            )
        )
        print("[Realtime] ì „ë‹¬ ìœ„ì¹˜ ì§ˆë¬¸ ì „ì†¡")

    async def _request_response(self, ws) -> None:
        await ws.send(
            json.dumps(
                {
                    "type": "response.create",
                    "response": {"modalities": self.modalities},
                }
            )
        )

    async def _drain_response(self, ws, *, stream_print: bool = False) -> str:
        transcript_parts: List[str] = []
        audio_chunks: List[str] = []
        printed = False
        while True:
            data = json.loads(await ws.recv())
            event_type = data.get("type")
            if self.debug_events:
                print(f"[Realtime:event] {event_type}")

            if event_type == "response.audio_transcript.delta":
                delta = data.get("delta", "")
                transcript_parts.append(delta)
                if stream_print and delta:
                    print(delta, end="", flush=True)
                    printed = True
            elif event_type in ("response.output_text.delta", "response.text.delta"):
                delta = data.get("delta", "")
                transcript_parts.append(delta)
                if stream_print and delta:
                    print(delta, end="", flush=True)
                    printed = True
            elif event_type == "response.audio.delta":
                audio_chunks.append(data.get("delta", ""))
            elif event_type == "response.audio.done" and self.play_audio and audio_chunks:
                await self._play_audio(audio_chunks)
                audio_chunks = []
            elif event_type == "response.done":
                if stream_print and printed:
                    print()
                return "".join(transcript_parts)
            elif event_type == "response.error":
                raise RuntimeError(data.get("error", {}).get("message", "Realtime response error"))

    async def _send_operator_turn(self, ws, turn: int) -> bool:
        if self.use_microphone:
            audio_b64 = await self._record_audio(self.operator_audio_seconds)
            if not audio_b64:
                return False
            await ws.send(json.dumps({"type": "input_audio_buffer.append", "audio": audio_b64}))
            await ws.send(json.dumps({"type": "input_audio_buffer.commit"}))
            return True

        text = input(f"Operator command #{turn+1}: ").strip()
        if not text:
            return False
        await ws.send(
            json.dumps(
                {
                    "type": "conversation.item.create",
                    "item": {
                        "type": "message",
                        "role": "user",
                        "content": [{"type": "input_text", "text": text}],
                    },
                }
            )
        )
        return True

    async def _record_audio(self, duration: int, fs: int = 24000) -> Optional[str]:
        try:
            print(f"\nğŸ™ï¸  Recording command for {duration}s... (Ctrl+C to skip)")
            recording = sd.rec(int(fs * duration), samplerate=fs, channels=1, dtype="int16")
            for i in range(duration):
                await asyncio.sleep(1)
                print(f"  {i + 1}/{duration}s", end="\r", flush=True)
            sd.wait()
            audio_b64 = base64.b64encode(recording.tobytes()).decode("utf-8")
            print("\nâœ… Audio captured\n")
            return audio_b64
        except Exception as exc:
            print(f"[WARN] Failed to capture audio: {exc}")
            return None

    async def _play_audio(self, chunks: Sequence[str], fs: int = 24000) -> None:
        joined = "".join(chunks)
        audio_bytes = base64.b64decode(joined)
        audio = np.frombuffer(audio_bytes, dtype=np.int16)
        sd.play(audio, samplerate=fs)
        sd.wait()


def _frame_to_data_url(frame_bgr: np.ndarray, quality: int = 90) -> str:
    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), quality]
    success, buffer = cv2.imencode(".jpg", frame_bgr, encode_param)
    if not success:
        raise RuntimeError("Failed to encode frame for realtime message")
    image_b64 = base64.b64encode(buffer.tobytes()).decode("utf-8")
    return f"data:image/jpeg;base64,{image_b64}"
